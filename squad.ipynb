{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv, json, string, re, time\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import chainer\n",
    "from chainer import Chain, Variable, Parameter\n",
    "from chainer import iterators, optimizers, serializers\n",
    "import chainer.initializers as I\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "WORD_VECTOR_SIZE = 300\n",
    "H_SIZE = 100\n",
    "POOL_SIZE = 16\n",
    "DROPOUT_RATE = 0.5\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = {}\n",
    "f = open('glove/glove.6B.' + str(WORD_VECTOR_SIZE) + 'd.txt', 'rb')\n",
    "reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    vector = map(float, row[1:])\n",
    "    glove[key] = np.array(vector, dtype=np.float32).reshape(1,-1)\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    textVec = np.array([])\n",
    "    for tok in tokens:\n",
    "        textVec = np.append(textVec, glove.get(tok, np.zeros((1,WORD_VECTOR_SIZE), dtype=np.float32)))\n",
    "    return textVec.reshape(1, -1)\n",
    "\n",
    "def answerpos(context, answer, answer_start):\n",
    "    start = len(word_tokenize(context[:answer_start]))\n",
    "    ans_len = len(word_tokenize(answer))\n",
    "    \n",
    "    return start, start + ans_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61379"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = []\n",
    "for jsonRow in json.loads(open('dataset/train.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']:\n",
    "        ctxVec = text2vec(paragraph['context'])\n",
    "        \n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qnVec = text2vec(qnaJson['question'])\n",
    "            \n",
    "            ansStart, ansEnd = answerpos(paragraph['context'], \n",
    "                                           qnaJson['answer']['text'], \n",
    "                                           qnaJson['answer']['answer_start'])\n",
    "            \n",
    "            train.append((ctxVec, qnVec, ansStart, ansEnd))\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 55379\n"
     ]
    }
   ],
   "source": [
    "shuffle(train)\n",
    "val = train[:6000]\n",
    "train = train[6000:]\n",
    "\n",
    "print len(val), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36790"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "for jsonRow in json.loads(open('dataset/test.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']:\n",
    "        ctx = paragraph['context']\n",
    "        ctxVec = text2vec(paragraph['context'])\n",
    "        \n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qnId = qnaJson['id']\n",
    "            qnVec = text2vec(qnaJson['question'])            \n",
    "            test.append((ctxVec, qnVec, qnId, ctx))\n",
    "  \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size, data):\n",
    "    j = min(i + batch_size, len(data))\n",
    "    \n",
    "    ctx = []\n",
    "    qn = []\n",
    "    ans_start = []\n",
    "    ans_end = []\n",
    "    \n",
    "    cmax = 0\n",
    "    qmax = 0\n",
    "    for k in range(i, j):\n",
    "        c, q, s, e = data[k]\n",
    "        ctx.append(c)\n",
    "        qn.append(q)\n",
    "        ans_start.append(s)\n",
    "        ans_end.append(e)\n",
    "        \n",
    "        cmax = max(cmax, c.shape[1])\n",
    "        qmax = max(qmax, q.shape[1])\n",
    "        \n",
    "    cVec = np.zeros((len(ctx), cmax), dtype=np.float32)\n",
    "    qVec = np.zeros((len(ctx), qmax), dtype=np.float32)        \n",
    "    for i in range(len(ctx)):\n",
    "        cVec[i, 0:ctx[i].shape[1]] = ctx[i]\n",
    "        qVec[i, 0:qn[i].shape[1]] = qn[i]\n",
    "    \n",
    "    return Variable(cVec), \\\n",
    "           Variable(qVec), \\\n",
    "           Variable(np.array(ans_start, dtype=np.int32)).reshape(-1,1), \\\n",
    "           Variable(np.array(ans_end, dtype=np.int32)).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 85200)\n",
      "(5, 4200)\n",
      "(5, 1)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "c, q, s, e = get_batch(0, 5, train)\n",
    "print c.shape\n",
    "print q.shape\n",
    "print s.shape\n",
    "print e.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "* RNN Tutorial: http://docs.chainer.org/en/stable/tutorial/recurrentnet.html\n",
    "* Training Tutorial: http://docs.chainer.org/en/stable/tutorial/train_loop.html\n",
    "* Attention: https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n",
    "* Pointer: http://fastml.com/introduction-to-pointer-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoattentionEncoder(Chain):\n",
    "    def __init__(self, wordvec_size, h_size, dropout_ratio, use_gpu=False):\n",
    "        super(CoattentionEncoder, self).__init__()\n",
    "        \n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.h_size = h_size\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.ctxRU = L.LSTM(wordvec_size, h_size)\n",
    "\n",
    "            self.qnRU = L.LSTM(wordvec_size, h_size)\n",
    "            self.qnLinear = L.Linear(h_size, h_size)\n",
    "            \n",
    "            self.outFwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outBwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outLinear = L.Linear(2*h_size, h_size)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"CodynamicAttention uses GPU\", self.use_gpu\n",
    "                self.ctxRU.to_gpu()\n",
    "                self.qnRU.to_gpu()\n",
    "                self.qnLinear.to_gpu()\n",
    "                self.outFwd.to_gpu()\n",
    "                self.outBwd.to_gpu()\n",
    "                self.outLinear.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.ctxRU.reset_state()\n",
    "        self.qnRU.reset_state()\n",
    "        self.outFwd.reset_state()\n",
    "        self.outBwd.reset_state()\n",
    "        \n",
    "    def get_para_rep(self, para, ru):\n",
    "        P = []\n",
    "        for i in range(0, para.shape[1], self.wordvec_size):\n",
    "            word = para[:, i:i+self.wordvec_size]\n",
    "            if self.use_gpu: \n",
    "                word.to_gpu()\n",
    "            P.append(F.dropout(ru(word), self.dropout_ratio))\n",
    "            \n",
    "        return F.transpose(F.dstack(P), (0, 1, 2))\n",
    "            \n",
    "    def __call__(self, ctx, qn):\n",
    "        # context representation\n",
    "        Ds = self.get_para_rep(ctx, self.ctxRU)\n",
    "        \n",
    "        #question representation\n",
    "        Qs = self.get_para_rep(qn, self.qnRU)\n",
    "        \n",
    "        out_ins = []\n",
    "        for i in range(Ds.shape[0]):\n",
    "            D = Ds[i]\n",
    "            Q = Qs[i]\n",
    "            \n",
    "            #attention\n",
    "            affinity = F.matmul(D.T, Q)\n",
    "            A_Q = F.softmax(affinity)\n",
    "            A_D = F.softmax(affinity.T)\n",
    "\n",
    "            C_Q = F.matmul(D, A_Q)\n",
    "            C_D = F.matmul(F.concat((Q, C_Q), axis=0), A_D)\n",
    "            \n",
    "            out_ins.append(F.concat((D, C_D), axis=0).T)\n",
    "        out_ins = F.transpose(F.dstack(out_ins), (0,2,1))\n",
    "\n",
    "        #output\n",
    "        h_fwd = []\n",
    "        for fout in out_ins:\n",
    "            h_fwd.append(F.dropout(self.outFwd(fout), self.dropout_ratio))\n",
    "        h_fwd = F.dstack(h_fwd)\n",
    "\n",
    "        h_bwd = []\n",
    "        for bout in out_ins[::-1]:\n",
    "            h_bwd.append(F.dropout(self.outBwd(bout), self.dropout_ratio))\n",
    "        h_bwd = F.dstack(h_bwd)\n",
    "        \n",
    "        u_in = F.transpose(F.concat((h_fwd, h_bwd)), (0,2,1))\n",
    "        U = F.dropout(self.outLinear(u_in.reshape(-1, 2*self.h_size)), self.dropout_ratio)\n",
    "        return U.reshape(Ds.shape[0], -1, self.h_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 284, 100)\n"
     ]
    }
   ],
   "source": [
    "ctx, qn, ans_start, ans_end = get_batch(0, 5, train)\n",
    "\n",
    "encoder = CoattentionEncoder(WORD_VECTOR_SIZE, H_SIZE, DROPOUT_RATE)\n",
    "\n",
    "U = encoder(ctx, qn)\n",
    "print U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(Chain):\n",
    "    def __init__(self, h_size, pool_size, dropout_ratio, use_gpu=False):\n",
    "        super(Highway, self).__init__()\n",
    "        \n",
    "        self.h_size = h_size\n",
    "        self.pool_size = pool_size\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.MLP = L.Linear(3*h_size, h_size, nobias=True)\n",
    "            self.M1 = L.Maxout(2*h_size, h_size, pool_size)\n",
    "            self.M2 = L.Maxout(h_size, h_size, pool_size)\n",
    "            self.M3 = L.Maxout(2*h_size, 1, pool_size)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"Highway uses GPU\", self.use_gpu\n",
    "                self.MLP.to_gpu()\n",
    "                self.M1.to_gpu()\n",
    "                self.M2.to_gpu()\n",
    "                self.M3.to_gpu()\n",
    "            \n",
    "    def __call__(self, U, h, us, ue):\n",
    "        if self.use_gpu:\n",
    "            U.to_gpu()\n",
    "            h.to_gpu()\n",
    "            us.to_gpu()\n",
    "            ue.to_gpu()\n",
    "        \n",
    "        r = F.tanh(self.MLP(F.hstack([h, us, ue])))\n",
    "        rs = []\n",
    "        for i in range(U.shape[0]):\n",
    "            rs.append(F.broadcast_to(r[i], U[i].shape))\n",
    "        r = F.transpose(F.dstack(rs), (2,0,1))\n",
    "        \n",
    "        m_in = F.concat((U, r), axis=2).reshape(-1, 2*self.h_size)\n",
    "        m1 = F.dropout(self.M1(m_in), self.dropout_ratio)\n",
    "        m2 = F.dropout(self.M2(m1), self.dropout_ratio)\n",
    "        m3 = self.M3(F.concat((m1,m2)))\n",
    "        \n",
    "        return m3.reshape(U.shape[0], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 284, 1)\n"
     ]
    }
   ],
   "source": [
    "highway = Highway(H_SIZE, POOL_SIZE, DROPOUT_RATE)\n",
    "\n",
    "h = Variable(np.zeros((5, H_SIZE), dtype=np.float32))\n",
    "us = U[:,0].reshape(5, -1)\n",
    "ue = U[:,-1].reshape(5, -1)\n",
    "\n",
    "alpha = highway(U, h, us, ue)\n",
    "print alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPointingDecoder(Chain):\n",
    "    def __init__(self, h_size, pool_size, dropout_ratio, use_gpu=False):\n",
    "        super(DynamicPointingDecoder, self).__init__()\n",
    "        \n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.dec_state = L.LSTM(2*h_size, h_size)\n",
    "            self.HwayStart = Highway(h_size, pool_size, dropout_ratio, use_gpu)\n",
    "            self.HwayEnd = Highway(h_size, pool_size, dropout_ratio, use_gpu)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                print \"DynamicPointincDecoded uses GPU\", self.use_gpu\n",
    "                self.dec_state.to_gpu()\n",
    "                self.HwayStart.to_gpu()\n",
    "                self.HwayEnd.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.dec_state.reset_state()\n",
    "            \n",
    "    def __call__(self, U, us, ue):\n",
    "        if self.use_gpu:\n",
    "            U.to_gpu()\n",
    "            us.to_gpu()\n",
    "            ue.to_gpu()\n",
    "        \n",
    "        h = F.dropout(self.dec_state(F.concat((us,ue))), self.dropout_ratio)\n",
    "        alpha = self.HwayStart(U, h, us, ue)\n",
    "        s = F.argmax(alpha, axis=1).data.reshape(-1)\n",
    "        beta = self.HwayEnd(U, h, U[range(U.shape[0]), s], ue)\n",
    "        \n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 284, 1) [[255 208 197 254 186]]\n",
      "(5, 284, 1) [[252 137 210  67 246]]\n"
     ]
    }
   ],
   "source": [
    "decoder = DynamicPointingDecoder(H_SIZE, POOL_SIZE, DROPOUT_RATE)\n",
    "\n",
    "alpha, beta = decoder(U, us, ue)\n",
    "print alpha.shape, F.argmax(alpha, axis=1).data.reshape(1,-1)\n",
    "print beta.shape, F.argmax(beta, axis=1).data.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadNet(Chain):\n",
    "    def __init__(self, wordvec_size, h_size, pool_size, dropout_rate, use_gpu=False):\n",
    "        super(SquadNet, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.encoder = CoattentionEncoder(wordvec_size, h_size, dropout_rate, use_gpu)\n",
    "            self.decoder = DynamicPointingDecoder(h_size, pool_size, dropout_rate, use_gpu)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"SquadNet uses GPU\", self.use_gpu\n",
    "                self.encoder.to_gpu()\n",
    "                self.decoder.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.encoder.reset_state()\n",
    "        self.decoder.reset_state()\n",
    "            \n",
    "    def __call__(self, ctx, qn): \n",
    "        U = self.encoder(ctx, qn)\n",
    "        \n",
    "        start = np.zeros(U.shape[0], 'i')\n",
    "        end = np.zeros(U.shape[0], 'i') - 1        \n",
    "        for i in range(4):            \n",
    "            us = U[range(U.shape[0]), start]\n",
    "            ue = U[range(U.shape[0]), end]\n",
    "            alpha, beta = self.decoder(U, us, ue)\n",
    "            \n",
    "            start = F.argmax(alpha, axis=1).data.reshape(-1)\n",
    "            end = F.argmax(beta, axis=1).data.reshape(-1)\n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 284, 1) [[ 52 190 197 154 133]]\n",
      "(5, 284, 1) [[192 108 198  88 105]]\n"
     ]
    }
   ],
   "source": [
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE, POOL_SIZE, DROPOUT_RATE)\n",
    "alpha, beta = model(ctx, qn)\n",
    "print alpha.shape, F.argmax(alpha, axis=1).data.reshape(1,-1)\n",
    "print beta.shape, F.argmax(beta, axis=1).data.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodynamicAttention uses GPU True\n",
      "Highway uses GPU True\n",
      "Highway uses GPU True\n",
      "DynamicPointincDecoded uses GPU True\n",
      "SquadNet uses GPU True\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.Adam(alpha=1e-3)\n",
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE, POOL_SIZE, DROPOUT_RATE, USE_GPU)\n",
    "if USE_GPU:\n",
    "    model.to_gpu()\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, epoch_start, epoch_end, batch_size, print_interval):\n",
    "    lossHistory = []\n",
    "    accHistory = []\n",
    "    for epoch in range(epoch_start, epoch_end):\n",
    "        print \"Epoch\", epoch + 1, \"/\", epoch_end\n",
    "        startTime = time.time()\n",
    "        epochScore = 0\n",
    "        epochLoss = 0\n",
    "\n",
    "        shuffle(train)\n",
    "        opt.new_epoch()\n",
    "        \n",
    "        interval_loss = 0\n",
    "        interval_size = 0\n",
    "        interval_start = time.time()\n",
    "        with chainer.using_config('train', True):\n",
    "            for i in range(0, len(train), batch_size):\n",
    "                try:\n",
    "                    ctx, qn, ans_start, ans_end = get_batch(i, batch_size, train)\n",
    "                    if USE_GPU:\n",
    "                        ans_start.to_gpu()\n",
    "                        ans_end.to_gpu()\n",
    "\n",
    "                    model.reset_state()\n",
    "                    pred_start, pred_end = model(ctx, qn)\n",
    "\n",
    "                    pred_start = pred_start[:ctx.shape[0],:,:]\n",
    "                    pred_end = pred_end[:ctx.shape[0]]\n",
    "\n",
    "                    loss_start = F.softmax_cross_entropy(pred_start, ans_start)\n",
    "                    loss_end = F.softmax_cross_entropy(pred_end, ans_end)\n",
    "                    loss = loss_start + loss_end\n",
    "\n",
    "                    interval_loss += loss.data * ctx.shape[0] \n",
    "                    interval_size += ctx.shape[0]\n",
    "                    epochLoss += loss.data * ctx.shape[0] / len(train)\n",
    "                    if i % print_interval == 0:\n",
    "                        print i, \"/\", len(train), \":\", \\\n",
    "                              interval_loss / interval_size, \\\n",
    "                              \"(\" + str(time.time() - interval_start) + \"s)\"\n",
    "                        interval_loss = 0\n",
    "                        interval_size = 0\n",
    "                        interval_start = time.time()\n",
    "\n",
    "                    s = F.argmax(pred_start, axis=1).data\n",
    "                    e = F.argmax(pred_end, axis=1).data\n",
    "                    for j in range(s.shape[0]):\n",
    "                        if s[j] == ans_start.data[j] and e[j] == ans_end.data[j]:\n",
    "                            epochScore += 1\n",
    "\n",
    "                    model.cleargrads()\n",
    "                    loss.backward()\n",
    "\n",
    "                    opt.update()\n",
    "                except IndexError as e:\n",
    "                    print \"Error on train index \" + str(i) + \":\", e\n",
    "        \n",
    "        valLoss = 0\n",
    "        valScore = 0\n",
    "        with chainer.using_config('train', False):\n",
    "            for i in range(0, len(val), batch_size):\n",
    "                try:\n",
    "                    ctx, qn, ans_start, ans_end = get_batch(i, batch_size, val)\n",
    "                    if USE_GPU:\n",
    "                        ans_start.to_gpu()\n",
    "                        ans_end.to_gpu()\n",
    "\n",
    "                    model.reset_state()\n",
    "                    pred_start, pred_end = model(ctx, qn)\n",
    "\n",
    "                    loss_start = F.softmax_cross_entropy(pred_start, ans_start)\n",
    "                    loss_end = F.softmax_cross_entropy(pred_end, ans_end)\n",
    "                    valLoss += (loss_start + loss_end).data  * ctx.shape[0] / len(val)\n",
    "\n",
    "                    s = F.argmax(pred_start, axis=1).data\n",
    "                    e = F.argmax(pred_end, axis=1).data\n",
    "                    for j in range(s.shape[0]):\n",
    "                        if s[j] == ans_start.data[j] and e[j] == ans_end.data[j]:\n",
    "                            valScore += 1\n",
    "                except IndexError as e:\n",
    "                    print \"Error on val index \" + str(i) + \":\", e\n",
    "        \n",
    "        epochAcc = float(epochScore) / len(train)\n",
    "        valAcc = float(valScore) / len(val)\n",
    "        \n",
    "        serializers.save_npz('gpu-epoch' + str(epoch+1) + '.model', model)\n",
    "        print \"Epoch completed in\", time.time() - startTime, \"seconds\"\n",
    "        print \"Train Acc:\", epochAcc, \"Val Acc:\", valAcc, \"Train Loss:\", epochLoss, \"Val Loss:\", valLoss\n",
    "        lossHistory.append((epochLoss, valLoss))\n",
    "        accHistory.append((epochAcc, valAcc))\n",
    "        \n",
    "        histFile = open('trainHist.txt', 'wb')\n",
    "        histFile.write(str(lossHistory) + '\\n')\n",
    "        histFile.write(str(accHistory) + '\\n')\n",
    "        histFile.close()\n",
    "        \n",
    "    return lossHistory, accHistory\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 15\n",
      "0 / 55379 : 12.098859787 (2.89157795906s)\n",
      "8000 / 55379 : 10.8223047256 (521.572746992s)\n",
      "16000 / 55379 : 9.64597988129 (540.985355139s)\n",
      "24000 / 55379 : 9.10246944427 (530.740674019s)\n",
      "32000 / 55379 : 8.68389797211 (507.199030876s)\n",
      "40000 / 55379 : 8.43607997894 (527.331501961s)\n",
      "48000 / 55379 : 8.17477416992 (512.578103065s)\n",
      "Epoch completed in 3731.38600898 seconds\n",
      "Train Acc: 0.0215424619441 Val Acc: 0.0828333333333 Train Loss: 8.985496521 Val Loss: 7.42939949036\n",
      "Epoch 2 / 15\n",
      "0 / 55379 : 7.83262348175 (2.11388087273s)\n",
      "8000 / 55379 : 7.7203373909 (514.315572977s)\n",
      "16000 / 55379 : 7.582862854 (517.7476089s)\n",
      "24000 / 55379 : 7.57368373871 (518.323364019s)\n",
      "32000 / 55379 : 7.45045614243 (509.801251888s)\n",
      "40000 / 55379 : 7.41792201996 (527.57490015s)\n",
      "48000 / 55379 : 7.37801742554 (543.573909998s)\n",
      "Epoch completed in 3725.06573391 seconds\n",
      "Train Acc: 0.0559056682136 Val Acc: 0.1125 Train Loss: 7.49291419983 Val Loss: 6.84998750687\n",
      "Epoch 3 / 15\n",
      "0 / 55379 : 7.04206371307 (1.9765150547s)\n",
      "8000 / 55379 : 7.1331911087 (541.303468943s)\n",
      "16000 / 55379 : 7.10278081894 (521.923694134s)\n",
      "24000 / 55379 : 7.03434038162 (464.323753119s)\n",
      "32000 / 55379 : 7.06194829941 (521.299188852s)\n",
      "40000 / 55379 : 7.07597351074 (515.258392811s)\n",
      "48000 / 55379 : 6.96670722961 (511.647981167s)\n",
      "Epoch completed in 3670.15696406 seconds\n",
      "Train Acc: 0.0779898517489 Val Acc: 0.130166666667 Train Loss: 7.05274915695 Val Loss: 6.56647825241\n",
      "Epoch 4 / 15\n",
      "0 / 55379 : 6.40526676178 (2.1263999939s)\n",
      "8000 / 55379 : 6.81467199326 (523.884304047s)\n",
      "16000 / 55379 : 6.78170681 (502.160959959s)\n",
      "24000 / 55379 : 6.75553178787 (543.751578093s)\n",
      "32000 / 55379 : 6.77187824249 (536.618347883s)\n",
      "40000 / 55379 : 6.73390722275 (529.359401941s)\n",
      "48000 / 55379 : 6.72381734848 (507.705416203s)\n",
      "Epoch completed in 3751.09658384 seconds\n",
      "Train Acc: 0.0908647682334 Val Acc: 0.137666666667 Train Loss: 6.75329494476 Val Loss: 6.39854335785\n",
      "Epoch 5 / 15\n",
      "0 / 55379 : 6.65963125229 (1.77279877663s)\n",
      "8000 / 55379 : 6.50099611282 (506.576254845s)\n",
      "16000 / 55379 : 6.55539941788 (522.810721874s)\n",
      "24000 / 55379 : 6.55964708328 (518.926409006s)\n",
      "32000 / 55379 : 6.62471723557 (529.311041117s)\n",
      "40000 / 55379 : 6.55875062943 (550.872117996s)\n",
      "48000 / 55379 : 6.56301689148 (542.903959036s)\n",
      "Epoch completed in 3768.94227505 seconds\n",
      "Train Acc: 0.104678668809 Val Acc: 0.149333333333 Train Loss: 6.54922914505 Val Loss: 6.28056287766\n",
      "Epoch 6 / 15\n",
      "0 / 55379 : 7.00392436981 (2.52790188789s)\n",
      "8000 / 55379 : 6.34974002838 (538.979138136s)\n",
      "16000 / 55379 : 6.26587724686 (515.848271132s)\n",
      "24000 / 55379 : 6.35081911087 (547.894077063s)\n",
      "32000 / 55379 : 6.36118984222 (529.670332193s)\n",
      "40000 / 55379 : 6.34511947632 (536.274466991s)\n",
      "48000 / 55379 : 6.34217262268 (530.079931974s)\n",
      "Epoch completed in 3787.76907301 seconds\n",
      "Train Acc: 0.114863034724 Val Acc: 0.155333333333 Train Loss: 6.32950496674 Val Loss: 6.18220806122\n",
      "Epoch 7 / 15\n",
      "0 / 55379 : 6.19775485992 (1.33153700829s)\n",
      "8000 / 55379 : 6.10827922821 (508.51257205s)\n",
      "16000 / 55379 : 6.14655017853 (544.936060905s)\n",
      "24000 / 55379 : 6.16574001312 (505.252775908s)\n",
      "32000 / 55379 : 6.11741781235 (496.200911999s)\n",
      "40000 / 55379 : 6.17760705948 (510.473462105s)\n",
      "48000 / 55379 : 6.17222499847 (510.184525013s)\n",
      "Epoch completed in 3688.30010986 seconds\n",
      "Train Acc: 0.124830712003 Val Acc: 0.157666666667 Train Loss: 6.15446901321 Val Loss: 6.08845329285\n",
      "Epoch 8 / 15\n",
      "0 / 55379 : 5.66130924225 (1.97640490532s)\n",
      "8000 / 55379 : 5.92795324326 (501.712157011s)\n",
      "16000 / 55379 : 5.99396133423 (509.292470932s)\n",
      "24000 / 55379 : 5.9911813736 (534.904069901s)\n",
      "32000 / 55379 : 6.00525665283 (519.256788015s)\n",
      "40000 / 55379 : 6.0187420845 (537.706322908s)\n",
      "48000 / 55379 : 6.00401210785 (552.940727949s)\n",
      "Epoch completed in 3760.74795699 seconds\n",
      "Train Acc: 0.133967749508 Val Acc: 0.164833333333 Train Loss: 5.98825454712 Val Loss: 6.0353884697\n",
      "Epoch 9 / 15\n",
      "0 / 55379 : 5.62902927399 (1.87157607079s)\n",
      "8000 / 55379 : 5.81702852249 (542.407423019s)\n",
      "16000 / 55379 : 5.79325866699 (496.642055035s)\n",
      "24000 / 55379 : 5.85010004044 (547.148370028s)\n",
      "32000 / 55379 : 5.87562131882 (543.878942966s)\n",
      "40000 / 55379 : 5.88150930405 (518.660714865s)\n",
      "48000 / 55379 : 5.85686159134 (533.324751139s)\n",
      "Epoch completed in 3797.18311715 seconds\n",
      "Train Acc: 0.140468408603 Val Acc: 0.170833333333 Train Loss: 5.85129356384 Val Loss: 5.97275257111\n",
      "Epoch 10 / 15\n",
      "0 / 55379 : 5.76568508148 (1.94250917435s)\n",
      "8000 / 55379 : 5.66762161255 (547.813628912s)\n",
      "16000 / 55379 : 5.69539546967 (541.194578886s)\n",
      "24000 / 55379 : 5.69415998459 (536.032469988s)\n",
      "32000 / 55379 : 5.69107723236 (560.159656048s)\n",
      "40000 / 55379 : 5.66963672638 (525.465582132s)\n",
      "48000 / 55379 : 5.70800876617 (532.994493008s)\n",
      "Epoch completed in 3850.371454 seconds\n",
      "Train Acc: 0.151230610881 Val Acc: 0.178833333333 Train Loss: 5.69365024567 Val Loss: 5.88764190674\n",
      "Epoch 11 / 15\n",
      "0 / 55379 : 5.21389627457 (1.70055413246s)\n",
      "8000 / 55379 : 5.53252363205 (546.679254055s)\n",
      "16000 / 55379 : 5.47828674316 (519.877576113s)\n",
      "24000 / 55379 : 5.54842472076 (532.365797997s)\n",
      "32000 / 55379 : 5.53420352936 (553.991379023s)\n",
      "40000 / 55379 : 5.60263204575 (559.655226946s)\n",
      "48000 / 55379 : 5.59377765656 (564.256494999s)\n",
      "Epoch completed in 3883.95288992 seconds\n",
      "Train Acc: 0.158128532476 Val Acc: 0.182 Train Loss: 5.55468225479 Val Loss: 5.86644744873\n",
      "Epoch 12 / 15\n",
      "0 / 55379 : 5.53113079071 (1.69447898865s)\n",
      "8000 / 55379 : 5.37176036835 (536.240648031s)\n",
      "16000 / 55379 : 5.41523456573 (528.995966911s)\n",
      "24000 / 55379 : 5.42136621475 (554.665753841s)\n",
      "32000 / 55379 : 5.39774751663 (547.58741498s)\n",
      "40000 / 55379 : 5.46335315704 (544.068626881s)\n",
      "48000 / 55379 : 5.43504571915 (527.172957897s)\n",
      "Epoch completed in 3880.65683603 seconds\n",
      "Train Acc: 0.168294841005 Val Acc: 0.183333333333 Train Loss: 5.42789268494 Val Loss: 5.88822555542\n",
      "Epoch 13 / 15\n",
      "0 / 55379 : 5.43660259247 (1.89048790932s)\n",
      "8000 / 55379 : 5.27056980133 (522.248142004s)\n",
      "16000 / 55379 : 5.2255897522 (561.926871061s)\n",
      "24000 / 55379 : 5.29813861847 (511.97716403s)\n",
      "32000 / 55379 : 5.29251909256 (540.211390018s)\n",
      "40000 / 55379 : 5.38009691238 (526.146178961s)\n",
      "48000 / 55379 : 5.34262990952 (517.438298941s)\n",
      "Epoch completed in 3830.35516405 seconds\n",
      "Train Acc: 0.175102475668 Val Acc: 0.188666666667 Train Loss: 5.31073522568 Val Loss: 5.85947942734\n",
      "Epoch 14 / 15\n",
      "0 / 55379 : 5.21794128418 (2.77358388901s)\n",
      "8000 / 55379 : 5.12270545959 (524.276276112s)\n",
      "16000 / 55379 : 5.14636421204 (523.511794806s)\n",
      "24000 / 55379 : 5.1751909256 (513.971112967s)\n",
      "32000 / 55379 : 5.18388271332 (524.949311018s)\n",
      "40000 / 55379 : 5.27705144882 (499.00769496s)\n",
      "48000 / 55379 : 5.24464464188 (478.333885908s)\n",
      "Epoch completed in 3680.32730293 seconds\n",
      "Train Acc: 0.182831037036 Val Acc: 0.184 Train Loss: 5.20293998718 Val Loss: 5.83757686615\n",
      "Epoch 15 / 15\n",
      "0 / 55379 : 5.29333782196 (1.65795588493s)\n",
      "8000 / 55379 : 5.00473642349 (533.093575001s)\n",
      "16000 / 55379 : 5.00013923645 (482.800889015s)\n",
      "24000 / 55379 : 5.07317638397 (537.530678988s)\n",
      "32000 / 55379 : 5.10122394562 (511.106698036s)\n",
      "48000 / 55379 : 5.18561601639 (524.06636095s)\n",
      "Epoch completed in 3734.31233883 seconds\n",
      "Train Acc: 0.187796818289 Val Acc: 0.191 Train Loss: 5.1040854454 Val Loss: 5.88537597656\n",
      "[(array(8.985496520996094, dtype=float32), array(7.429399490356445, dtype=float32)), (array(7.492914199829102, dtype=float32), array(6.849987506866455, dtype=float32)), (array(7.052749156951904, dtype=float32), array(6.566478252410889, dtype=float32)), (array(6.753294944763184, dtype=float32), array(6.398543357849121, dtype=float32)), (array(6.549229145050049, dtype=float32), array(6.280562877655029, dtype=float32)), (array(6.32950496673584, dtype=float32), array(6.182208061218262, dtype=float32)), (array(6.154469013214111, dtype=float32), array(6.08845329284668, dtype=float32)), (array(5.988254547119141, dtype=float32), array(6.035388469696045, dtype=float32)), (array(5.851293563842773, dtype=float32), array(5.972752571105957, dtype=float32)), (array(5.693650245666504, dtype=float32), array(5.887641906738281, dtype=float32)), (array(5.55468225479126, dtype=float32), array(5.866447448730469, dtype=float32)), (array(5.427892684936523, dtype=float32), array(5.888225555419922, dtype=float32)), (array(5.31073522567749, dtype=float32), array(5.8594794273376465, dtype=float32)), (array(5.202939987182617, dtype=float32), array(5.837576866149902, dtype=float32)), (array(5.104085445404053, dtype=float32), array(5.8853759765625, dtype=float32))]\n",
      "[(0.021542461944058216, 0.08283333333333333), (0.05590566821358277, 0.1125), (0.07798985174885786, 0.13016666666666668), (0.09086476823344589, 0.13766666666666666), (0.10467866880947652, 0.14933333333333335), (0.114863034724354, 0.15533333333333332), (0.12483071200274472, 0.15766666666666668), (0.13396774950793622, 0.16483333333333333), (0.14046840860253887, 0.17083333333333334), (0.15123061088138104, 0.17883333333333334), (0.1581285324762094, 0.182), (0.16829484100471298, 0.18333333333333332), (0.17510247566767187, 0.18866666666666668), (0.18283103703569945, 0.184), (0.1877968182885209, 0.191)]\n"
     ]
    }
   ],
   "source": [
    "serializers.load_npz('gpu-epoch15.model', model)\n",
    "lossHistory, accHistory = train_model(model, opt, 0, 15, 80, 8000)\n",
    "\n",
    "print lossHistory\n",
    "print accHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(i, batch_size, data):\n",
    "    j = min(i + batch_size, len(data))\n",
    "    \n",
    "    ctx = []\n",
    "    qn = []\n",
    "    ids = []\n",
    "    ctxStrs = []\n",
    "    \n",
    "    cmax = 0\n",
    "    qmax = 0\n",
    "    for k in range(i, j):\n",
    "        c, q, s, e = data[k]\n",
    "        ctx.append(c)\n",
    "        qn.append(q)\n",
    "        ids.append(s)\n",
    "        ctxStrs.append(e)\n",
    "        \n",
    "        cmax = max(cmax, c.shape[1])\n",
    "        qmax = max(qmax, q.shape[1])\n",
    "\n",
    "    cVec = np.zeros((len(ctx), cmax), dtype=np.float32)\n",
    "    qVec = np.zeros((len(ctx), qmax), dtype=np.float32)        \n",
    "    for i in range(len(ctx)):\n",
    "        cVec[i, 0:ctx[i].shape[1]] = ctx[i]\n",
    "        qVec[i, 0:qn[i].shape[1]] = qn[i]\n",
    "    \n",
    "    return Variable(cVec), \\\n",
    "           Variable(qVec), \\\n",
    "           ids, \\\n",
    "           ctxStrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_batch_size, test_print_interval, model_file,out_file):    \n",
    "    serializers.load_npz(model_file, model)\n",
    "\n",
    "    f = open(out_file, 'wb')\n",
    "    out = csv.writer(f)\n",
    "    out.writerow([\"Id\", \"Answer\"])\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    with chainer.using_config('train', False):\n",
    "        for i in range(0, len(test), test_batch_size):\n",
    "            ctx, qn, qnId, ctxStr = get_test_batch(i, test_batch_size, test)\n",
    "            model.reset_state()\n",
    "            start, end = model(ctx, qn)\n",
    "\n",
    "            for j in range(len(qnId)):\n",
    "                contextTokens = word_tokenize(ctxStr[j])\n",
    "\n",
    "                s = F.argmax(start[j]).data\n",
    "                e = F.argmax(end[j]).data\n",
    "\n",
    "                s = min(s, len(contextTokens)-1)\n",
    "                e = max(e, s)\n",
    "                e = min(e, len(contextTokens)-1)        \n",
    "\n",
    "                ans = \"\"\n",
    "                for k in range(s, e + 1):\n",
    "                    ans += contextTokens[k] + \" \"\n",
    "\n",
    "                out.writerow([qnId[j], normalize_answer(ans).encode('utf-8')])\n",
    "\n",
    "            if i % test_print_interval == 0:\n",
    "                print i, \"/\", len(test), \"(\" + str(time.time() - startTime) + \"s)\"\n",
    "                startTime = time.time()\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 36790 (1.48956084251s)\n",
      "8000 / 36790 (134.038506985s)\n",
      "16000 / 36790 (136.514800072s)\n",
      "24000 / 36790 (132.842578888s)\n",
      "32000 / 36790 (138.968399048s)\n",
      "0 / 36790 (1.35005712509s)\n",
      "8000 / 36790 (136.725776911s)\n",
      "16000 / 36790 (138.117974997s)\n",
      "24000 / 36790 (134.82595396s)\n",
      "32000 / 36790 (141.707485914s)\n",
      "0 / 36790 (1.7021780014s)\n",
      "8000 / 36790 (138.867139101s)\n",
      "16000 / 36790 (141.737091064s)\n",
      "24000 / 36790 (134.917159081s)\n",
      "32000 / 36790 (138.22132206s)\n",
      "0 / 36790 (1.67653608322s)\n",
      "8000 / 36790 (137.888603926s)\n",
      "16000 / 36790 (138.90299511s)\n",
      "24000 / 36790 (134.337846041s)\n",
      "32000 / 36790 (141.028888941s)\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 80\n",
    "test_print_interval = 8000\n",
    "\n",
    "test_model(test_batch_size, test_print_interval, 'gpu-epoch5.model','pred5.csv')\n",
    "test_model(test_batch_size, test_print_interval, 'gpu-epoch7.model','pred7.csv')\n",
    "test_model(test_batch_size, test_print_interval, 'gpu-epoch10.model','pred10.csv')\n",
    "test_model(test_batch_size, test_print_interval, 'gpu-epoch15.model','pred15.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
