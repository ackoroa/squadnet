{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv, json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from chainer import Chain, Variable, Parameter\n",
    "import chainer.optimizers\n",
    "import chainer.initializers as I\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "WORD_VECTOR_SIZE = 50\n",
    "H_SIZE = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = {}\n",
    "f = open('glove/glove.6B.' + str(WORD_VECTOR_SIZE) + 'd.txt', 'rb')\n",
    "reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    vector = map(float, row[1:])\n",
    "    glove[key] = np.array(vector, dtype=np.float32).reshape(1,-1)\n",
    "glove['eos'] = 10 * np.ones((1, WORD_VECTOR_SIZE), dtype=np.float32)\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<eos>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-ff6fde7a7906>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: '<eos>'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    textVec = []\n",
    "    for tok in tokens:\n",
    "        textVec.append(glove.get(tok, np.zeros((1,WORD_VECTOR_SIZE), dtype=np.float32)))\n",
    "    return textVec\n",
    "\n",
    "def answerpos(context, answer, answer_start):\n",
    "    start = len(word_tokenize(context[:answer_start]))\n",
    "    ans_len = len(word_tokenize(answer))\n",
    "    \n",
    "    return start, start + ans_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12979\n",
      "In 1790, the first federal population census was taken in the United States. Enumerators were instructed to classify free residents as white or \"other.\" Only the heads of households were identified by name in the federal census until 1850. Native Americans were included among \"Other;\" in later censuses, they were included as \"Free people of color\" if they were not living on Indian reservations. Slaves were counted separately from free persons in all the censuses until the Civil War and end of slavery. In later censuses, people of African descent were classified by appearance as mulatto (which recognized visible European ancestry in addition to African) or black.\n",
      "In 1790, the first federal population census was taken in the United States\n",
      "0 In\n",
      "13 States\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for jsonRow in json.loads(open('dataset/train.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']: \n",
    "        data = {}\n",
    "        data['context'] = paragraph['context']\n",
    "        data['contextVec'] = text2vec(paragraph['context'])\n",
    "        \n",
    "        data['qna'] = []\n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qna = {}\n",
    "            qna['question'] = qnaJson['question']\n",
    "            qna['questionVec'] = text2vec(qnaJson['question'].lower())\n",
    "            \n",
    "            qna['answer'] = qnaJson['answer']['text']\n",
    "            ans_start, ans_end = answerpos(paragraph['context'], \n",
    "                                           qnaJson['answer']['text'], \n",
    "                                           qnaJson['answer']['answer_start'])\n",
    "            qna['answer_start'] = ans_start\n",
    "            qna['answer_end'] = ans_end\n",
    "            \n",
    "            data['qna'].append(qna)\n",
    "        train.append(data)\n",
    "print len(train)\n",
    "\n",
    "idx = 4\n",
    "print train[0]['context']\n",
    "print train[0]['qna'][idx]['answer']\n",
    "print train[0]['qna'][idx]['answer_start'], word_tokenize(train[0]['context'])[train[0]['qna'][idx]['answer_start']]\n",
    "print train[0]['qna'][idx]['answer_end'], word_tokenize(train[0]['context'])[train[0]['qna'][idx]['answer_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7984"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "for jsonRow in json.loads(open('dataset/test.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']: \n",
    "        data = {}\n",
    "        data['context'] = paragraph['context']\n",
    "        data['contextVec'] = text2vec(paragraph['context'])\n",
    "        data['qna'] = []\n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qna = {}\n",
    "            qna['question'] = qnaJson['question']\n",
    "            qna['questionVec'] = text2vec(qnaJson['question'].lower())\n",
    "            data['qna'].append(qna)\n",
    "        test.append(data)\n",
    "\n",
    "print len(test)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "* RNN Tutorial: http://docs.chainer.org/en/stable/tutorial/recurrentnet.html\n",
    "* Training Tutorial: http://docs.chainer.org/en/stable/tutorial/train_loop.html\n",
    "* Attention: https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n",
    "* Pointer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CoattentionEncoder(Chain):\n",
    "    def __init__(self, wordvec_size, h_size):\n",
    "        super(CoattentionEncoder, self).__init__()\n",
    "        \n",
    "        self.h_size = h_size\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.ctxRU = L.LSTM(wordvec_size, h_size)\n",
    "\n",
    "            self.qnRU = L.LSTM(wordvec_size, h_size)\n",
    "            self.qnLinear = L.Linear(h_size, h_size)\n",
    "            \n",
    "            self.outFwd = L.LSTM(None, h_size)\n",
    "            self.outBwd = L.LSTM(None, h_size)\n",
    "            self.outLinear = L.Linear(h_size*2, h_size)            \n",
    "            \n",
    "    def __call__(self, context, question):\n",
    "        # context representation\n",
    "        D = []\n",
    "        for word in context:\n",
    "            D.append(self.ctxRU(word))\n",
    "        D = F.vstack(D).T\n",
    "        \n",
    "        #question representation\n",
    "        Q = []\n",
    "        for word in question:\n",
    "            Q.append(self.qnRU(word))\n",
    "        Q = self.qnLinear(F.vstack(Q)).T\n",
    "        \n",
    "        #attention\n",
    "        affinity = F.matmul(D.T, Q)\n",
    "        A_Q = F.softmax(affinity)\n",
    "        A_D = F.softmax(affinity.T)\n",
    "        \n",
    "        C_Q = F.matmul(D, A_Q)\n",
    "        C_D = F.matmul(F.concat((Q, C_Q), axis=0), A_D)\n",
    "        \n",
    "        #output\n",
    "        out_in = F.concat((D, C_D), axis=0).T\n",
    "        \n",
    "        h_fwd = []\n",
    "        for fout in out_in:\n",
    "            h_fwd.append(self.outFwd(fout.reshape(1,-1)))\n",
    "        h_fwd = F.vstack(h_fwd)\n",
    "        \n",
    "        h_bwd = []\n",
    "        for bout in out_in[::-1]:\n",
    "            h_bwd.append(self.outBwd(bout.reshape(1,-1)))\n",
    "        h_bwd = F.vstack(h_bwd)\n",
    "    \n",
    "        U = self.outLinear(F.concat((h_fwd, h_bwd)))\n",
    "        \n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicPointingDecoder(Chain):\n",
    "    def __init__(self):\n",
    "        super(DynamicPointingDecoder, self).__init__()\n",
    "                \n",
    "        with self.init_scope():\n",
    "            \n",
    "            \n",
    "    def __call__(self, context, question):\n",
    "        start, end\n",
    "        return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 35)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = train[0]['contextVec']\n",
    "qn = train[0]['qna'][0]['questionVec']\n",
    "model = CoattentionEncoder(WORD_VECTOR_SIZE, H_SIZE, 0)\n",
    "\n",
    "model(ctx, qn).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
