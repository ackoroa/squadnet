{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv, json, string, re, time\n",
    "\n",
    "import numpy as np\n",
    "import cupy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import chainer\n",
    "from chainer import Chain, Variable, Parameter\n",
    "from chainer import iterators, optimizers\n",
    "import chainer.initializers as I\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "WORD_VECTOR_SIZE = 300\n",
    "H_SIZE = 200\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = {}\n",
    "f = open('glove/glove.6B.' + str(WORD_VECTOR_SIZE) + 'd.txt', 'rb')\n",
    "reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    vector = map(float, row[1:])\n",
    "    glove[key] = np.array(vector, dtype=np.float32).reshape(1,-1)\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    textVec = []\n",
    "    for tok in tokens:\n",
    "        textVec.append(glove.get(tok, np.zeros((1,WORD_VECTOR_SIZE), dtype=np.float32)))\n",
    "    return textVec\n",
    "\n",
    "def answerpos(context, answer, answer_start):\n",
    "    start = len(word_tokenize(context[:answer_start]))\n",
    "    ans_len = len(word_tokenize(answer))\n",
    "    \n",
    "    return start, start + ans_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61379\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for jsonRow in json.loads(open('dataset/train.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']: \n",
    "        ctxLen = len(paragraph['context'])\n",
    "        ctxVec = text2vec(paragraph['context'])\n",
    "        \n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qnVec = text2vec(qnaJson['question'].lower())\n",
    "            \n",
    "            ansStart, ansEnd = answerpos(paragraph['context'], \n",
    "                                           qnaJson['answer']['text'], \n",
    "                                           qnaJson['answer']['answer_start'])\n",
    "            \n",
    "            train.append((ctxVec, qnVec, ansStart, ansEnd))\n",
    "len_train = len(train)\n",
    "print len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for jsonRow in json.loads(open('dataset/test.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']: \n",
    "        data = {}\n",
    "        data['context'] = paragraph['context']\n",
    "        data['contextVec'] = text2vec(paragraph['context'])\n",
    "        data['qna'] = []\n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qna = {}\n",
    "            qna['id'] = qnaJson['id']\n",
    "            qna['question'] = qnaJson['question']\n",
    "            qna['questionVec'] = text2vec(qnaJson['question'].lower())\n",
    "            data['qna'].append(qna)\n",
    "        test.append(data)\n",
    "\n",
    "print len(test)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "* RNN Tutorial: http://docs.chainer.org/en/stable/tutorial/recurrentnet.html\n",
    "* Training Tutorial: http://docs.chainer.org/en/stable/tutorial/train_loop.html\n",
    "* Attention: https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n",
    "* Pointer: http://fastml.com/introduction-to-pointer-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoattentionEncoder(Chain):\n",
    "    def __init__(self, wordvec_size, h_size, use_gpu=False):\n",
    "        super(CoattentionEncoder, self).__init__()\n",
    "        \n",
    "        self.h_size = h_size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.ctxRU = L.LSTM(wordvec_size, h_size)\n",
    "\n",
    "            self.qnRU = L.LSTM(wordvec_size, h_size)\n",
    "            self.qnLinear = L.Linear(h_size, h_size)\n",
    "            \n",
    "            self.outFwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outBwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outLinear = L.Linear(2*h_size, h_size)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"CodynamicAttention uses GPU\", self.use_gpu\n",
    "                self.ctxRU.to_gpu()\n",
    "                self.qnRU.to_gpu()\n",
    "                self.qnLinear.to_gpu()\n",
    "                self.outFwd.to_gpu()\n",
    "                self.outBwd.to_gpu()\n",
    "                self.outLinear.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.ctxRU.reset_state()\n",
    "        self.qnRU.reset_state()\n",
    "        self.outFwd.reset_state()\n",
    "        self.outBwd.reset_state()\n",
    "            \n",
    "    def __call__(self, context, question):\n",
    "        # context representation\n",
    "        D = []\n",
    "        for word in ctx:\n",
    "            if self.use_gpu:\n",
    "                word = Variable(word)\n",
    "                word.to_gpu()\n",
    "            D.append(self.ctxRU(word))\n",
    "        D = F.vstack(D).T\n",
    "        \n",
    "        #question representation\n",
    "        Q = []\n",
    "        for word in qn:\n",
    "            if self.use_gpu:\n",
    "                word = Variable(word)\n",
    "                word.to_gpu()\n",
    "            Q.append(self.qnRU(word))\n",
    "        Q = F.vstack(Q).T\n",
    "        \n",
    "        #attention\n",
    "        affinity = F.matmul(D.T, Q)\n",
    "        A_Q = F.softmax(affinity)\n",
    "        A_D = F.softmax(affinity.T)\n",
    "        \n",
    "        C_Q = F.matmul(D, A_Q)\n",
    "        C_D = F.matmul(F.concat((Q, C_Q), axis=0), A_D)\n",
    "        \n",
    "        #output\n",
    "        out_in = F.concat((D, C_D), axis=0).T \n",
    "        h_fwd = []\n",
    "        for fout in out_in:\n",
    "            h_fwd.append(self.outFwd(fout.reshape(1,-1)))\n",
    "        h_fwd = F.vstack(h_fwd)\n",
    "            \n",
    "        h_bwd = []\n",
    "        for bout in out_in[::-1]:\n",
    "            h_bwd.append(self.outBwd(bout.reshape(1,-1)))\n",
    "        h_bwd = F.vstack(h_bwd)\n",
    "\n",
    "        U = self.outLinear(F.concat((h_fwd, h_bwd)))\n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 200)\n",
      "variable([[-0.01048665  0.00064355 -0.01285294 ...,  0.01555272 -0.03691786\n",
      "           -0.02425362]\n",
      "          [-0.0051848   0.0156891   0.00302843 ...,  0.03482651 -0.05149583\n",
      "           -0.05101404]\n",
      "          [ 0.00164901  0.02376289  0.02370892 ...,  0.03921724 -0.06495478\n",
      "           -0.03598216]\n",
      "          ..., \n",
      "          [ 0.23352769  0.12652838  0.07524005 ...,  0.05326129 -0.09351344\n",
      "           -0.01281496]\n",
      "          [ 0.23967445  0.11973716  0.05234898 ...,  0.03714702 -0.10802287\n",
      "           -0.01751066]\n",
      "          [ 0.24274108  0.11369845  0.04697984 ...,  0.01832231 -0.11674075\n",
      "           -0.02041944]])\n"
     ]
    }
   ],
   "source": [
    "ctx = train[0][0]\n",
    "qn = train[0][1]\n",
    "encoder = CoattentionEncoder(WORD_VECTOR_SIZE, H_SIZE)\n",
    "\n",
    "U = encoder(ctx, qn)\n",
    "print U.shape\n",
    "print U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(Chain):\n",
    "    def __init__(self, h_size, use_gpu=False):\n",
    "        super(Highway, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.MLP = L.Linear(3*h_size, h_size, nobias=True)\n",
    "            self.M1 = L.Linear(2*h_size, h_size)\n",
    "            self.M2 = L.Linear(h_size, h_size)\n",
    "            self.M3 = L.Linear(2*h_size, 1)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"Highway uses GPU\", self.use_gpu\n",
    "                self.MLP.to_gpu()\n",
    "                self.M1.to_gpu()\n",
    "                self.M2.to_gpu()\n",
    "                self.M3.to_gpu()\n",
    "            \n",
    "    def __call__(self, U, h, us, ue):\n",
    "        if self.use_gpu:\n",
    "            U.to_gpu()\n",
    "            h.to_gpu()\n",
    "            us.to_gpu()\n",
    "            ue.to_gpu()\n",
    "        \n",
    "        r = F.tanh(self.MLP(F.hstack([h, us, ue])))\n",
    "        m1 = self.M1(F.concat((U, F.broadcast_to(r, U.shape))))\n",
    "        m2 = self.M2(m1)\n",
    "        m3 = self.M3(F.concat((m1,m2)))\n",
    "        \n",
    "        return m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 1)\n"
     ]
    }
   ],
   "source": [
    "highway = Highway(H_SIZE)\n",
    "\n",
    "h = Variable(np.zeros((1,H_SIZE), dtype=np.float32))\n",
    "us = U[0].reshape(1,-1)\n",
    "ue = U[-1].reshape(1,-1)\n",
    "\n",
    "alpha = highway(U, h, us, ue)\n",
    "print alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPointingDecoder(Chain):\n",
    "    def __init__(self, h_size, use_gpu=False):\n",
    "        super(DynamicPointingDecoder, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.dec_state = L.LSTM(2*h_size, h_size)\n",
    "            self.HwayStart = Highway(h_size, use_gpu)\n",
    "            self.HwayEnd = Highway(h_size, use_gpu)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                print \"DynamicPointincDecoded uses GPU\", self.use_gpu\n",
    "                self.dec_state.to_gpu()\n",
    "                self.HwayStart.to_gpu()\n",
    "                self.HwayEnd.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.dec_state.reset_state()\n",
    "            \n",
    "    def __call__(self, U, us, ue):\n",
    "        if self.use_gpu:\n",
    "            U.to_gpu()\n",
    "            us.to_gpu()\n",
    "            ue.to_gpu()\n",
    "        \n",
    "        h = self.dec_state(F.concat((us,ue)))\n",
    "        alpha = self.HwayStart(U, h, us, ue)\n",
    "        s = F.argmax(alpha).data\n",
    "        beta = self.HwayEnd(U, h, U[s].reshape(1,-1), ue)\n",
    "        \n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 1) variable(0)\n",
      "(125, 1) variable(94)\n"
     ]
    }
   ],
   "source": [
    "decoder = DynamicPointingDecoder(H_SIZE)\n",
    "\n",
    "us = U[0].reshape(1,-1)\n",
    "ue = U[-1].reshape(1,-1)\n",
    "alpha, beta = decoder(U, us, ue)\n",
    "print alpha.shape, F.argmax(alpha)\n",
    "print beta.shape, F.argmax(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadNet(Chain):\n",
    "    def __init__(self, wordvec_size, h_size, use_gpu=False):\n",
    "        super(SquadNet, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.encoder = CoattentionEncoder(wordvec_size, h_size, use_gpu)\n",
    "            self.decoder = DynamicPointingDecoder(h_size, use_gpu)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"SquadNet uses GPU\", self.use_gpu\n",
    "                self.encoder.to_gpu()\n",
    "                self.decoder.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.encoder.reset_state()\n",
    "        self.decoder.reset_state()\n",
    "            \n",
    "    def __call__(self, ctx, qn):        \n",
    "        U = self.encoder(ctx, qn)\n",
    "        \n",
    "        start = 0\n",
    "        end = -1\n",
    "        for i in range(3):            \n",
    "            us = U[start].reshape(1, -1)\n",
    "            ue = U[end].reshape(1, -1)\n",
    "            alpha, beta = self.decoder(U, us, ue)\n",
    "            \n",
    "            start = F.argmax(alpha).data\n",
    "            end = F.argmax(beta).data\n",
    "        return alpha.reshape(1, -1), beta.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 1)\n",
      "(125, 1)\n"
     ]
    }
   ],
   "source": [
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE)\n",
    "alpha. beta = model(ctx, qn)\n",
    "print alpha.shape\n",
    "print beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodynamicAttention uses GPU True\n",
      "Highway uses GPU True\n",
      "Highway uses GPU True\n",
      "DynamicPointincDecoded uses GPU True\n",
      "SquadNet uses GPU True\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.Adam()\n",
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE, USE_GPU)\n",
    "if USE_GPU:\n",
    "    model.to_gpu()\n",
    "\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size):\n",
    "    j = min(i + batch_size, len_train)\n",
    "    \n",
    "    ctx = []\n",
    "    qn = []\n",
    "    ans_start = []\n",
    "    ans_end = []\n",
    "    for k in range(i, j):\n",
    "        c, q, s, e = train[k]\n",
    "        ctx.append(c)\n",
    "        qn.append(q)\n",
    "        ans_start.append(s)\n",
    "        ans_end.append(e)\n",
    "    \n",
    "    return np.vstack(ctx), np.vstack(qn), Variable(np.array(ans_start)), Variable(np.array(ans_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\n",
      "0 / 61379 : 9.6827583313 (1.18510293961s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-95ef34336ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mpred_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mansStartVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-6d06ab9d0dc3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ctx, qn)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e41d888c3e5d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, context, question)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mh_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mh_fwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutFwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mh_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_fwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schorcingfalcon/.local/lib/python2.7/site-packages/chainer/links/connection/lstm.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mlstm_in\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlateral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mlstm_in\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlateral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schorcingfalcon/.local/lib/python2.7/site-packages/chainer/links/connection/linear.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/schorcingfalcon/.local/lib/python2.7/site-packages/chainer/functions/connection/linear.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(x, W, b)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schorcingfalcon/.local/lib/python2.7/site-packages/chainer/function_node.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_backprop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# Topological ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_vars\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;31m# Add backward edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCH = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PRINT_INTERVAL = 1000\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    print \"Epoch\", epoch + 1, \"/\", N_EPOCH\n",
    "    startTime = time.time()\n",
    "    \n",
    "    interval_loss = 0\n",
    "    interval_start = time.time()\n",
    "    for i in range(0, len_train, BATCH_SIZE):\n",
    "        try:\n",
    "            ctx, qn, ans_start, ans_end = get_batch(i, BATCH_SIZE)\n",
    "                        \n",
    "            model.reset_state()\n",
    "            pred_start, pred_end = model(ctx, qn)\n",
    "            \n",
    "            if USE_GPU:\n",
    "                ans_start.to_gpu()\n",
    "                ans_end.to_gpu()\n",
    "            \n",
    "            loss_start = F.softmax_cross_entropy(pred_start, ans_start)\n",
    "            loss_end = F.softmax_cross_entropy(pred_end, ans_end)\n",
    "            loss = loss_start + loss_end\n",
    "            \n",
    "            interval_loss += loss.data\n",
    "            if i % PRINT_INTERVAL == 0:\n",
    "                print i, \"/\", len_train, \":\", interval_loss, \"(\" + str(time.time() - interval_start) + \"s)\"\n",
    "                interval_loss = 0\n",
    "                interval_start = time.time()\n",
    "\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            opt.update()\n",
    "        except IndexError as e:\n",
    "            print \"Error on index \" + str(i) + \":\", e\n",
    "    print \"Epoch completed in\", time.time() - startTime, \"seconds\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pred.csv', 'wb')\n",
    "out = csv.writer(f)\n",
    "out.writerow([\"Id\", \"Answer\"])\n",
    "\n",
    "for t in test:\n",
    "    ctx = word_tokenize(t['context'])\n",
    "    ctxVec = t['contextVec']\n",
    "    \n",
    "    for qna in t['qna']:\n",
    "        id = qna['id']\n",
    "        qn = qna['question']\n",
    "        qnVec = qna['questionVec']\n",
    "        \n",
    "        model.reset_state()\n",
    "        start, end = model(ctxVec, qnVec)\n",
    "        start = F.argmax(start).data\n",
    "        end = F.argmax(end).data\n",
    "        \n",
    "        ans = \"\"\n",
    "        for i in range(start, end + 1):\n",
    "            ans += ctx[i] + \" \"\n",
    "        \n",
    "        out.writerow([id, normalize_answer(ans).encode('utf-8')])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
