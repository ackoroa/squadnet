{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv, json, string, re, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import chainer\n",
    "from chainer import Chain, Variable, Parameter\n",
    "from chainer import iterators, optimizers, serializers\n",
    "import chainer.initializers as I\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "WORD_VECTOR_SIZE = 50\n",
    "H_SIZE = 35\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = {}\n",
    "f = open('glove/glove.6B.' + str(WORD_VECTOR_SIZE) + 'd.txt', 'rb')\n",
    "reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    vector = map(float, row[1:])\n",
    "    glove[key] = np.array(vector, dtype=np.float32).reshape(1,-1)\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    textVec = np.array([])\n",
    "    for tok in tokens:\n",
    "        textVec = np.append(textVec, glove.get(tok, np.zeros((1,WORD_VECTOR_SIZE), dtype=np.float32)))\n",
    "    return textVec.reshape(1, -1)\n",
    "\n",
    "def answerpos(context, answer, answer_start):\n",
    "    start = len(word_tokenize(context[:answer_start]))\n",
    "    ans_len = len(word_tokenize(answer))\n",
    "    \n",
    "    return start, start + ans_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = []\n",
    "for jsonRow in json.loads(open('dataset/train.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']:\n",
    "        ctxVec = text2vec(paragraph['context'])\n",
    "        \n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qnVec = text2vec(qnaJson['question']\n",
    "            \n",
    "            ansStart, ansEnd = answerpos(paragraph['context'], \n",
    "                                           qnaJson['answer']['text'], \n",
    "                                           qnaJson['answer']['answer_start'])\n",
    "            \n",
    "            train.append((ctxVec, qnVec, ansStart, ansEnd))\n",
    "\n",
    "train = np.array(train)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 4) (55379, 4)\n"
     ]
    }
   ],
   "source": [
    "shuf = np.random.permutation(len(train))\n",
    "val = train[shuf[:6000]]\n",
    "train = train[shuf[6000:]]\n",
    "\n",
    "print val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36790, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "for jsonRow in json.loads(open('dataset/test.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']:\n",
    "        ctx = paragraph['context']\n",
    "        ctxVec = text2vec(paragraph['context'])\n",
    "        \n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qnId = qnaJson['id']\n",
    "            qnVec = text2vec(qnaJson['question'])            \n",
    "            test.append((ctxVec, qnVec, qnId, ctx))\n",
    "            \n",
    "test = np.array(test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size, data):\n",
    "    j = min(i + batch_size, len(data))\n",
    "    \n",
    "    ctx = []\n",
    "    qn = []\n",
    "    ans_start = []\n",
    "    ans_end = []\n",
    "    \n",
    "    cmax = 0\n",
    "    qmax = 0\n",
    "    for k in range(i, j):\n",
    "        c, q, s, e = data[k]\n",
    "        ctx.append(c)\n",
    "        qn.append(q)\n",
    "        ans_start.append(s)\n",
    "        ans_end.append(e)\n",
    "        \n",
    "        cmax = max(cmax, c.shape[1])\n",
    "        qmax = max(cmax, q.shape[1])\n",
    "\n",
    "    cVec = np.zeros((batch_size, cmax), dtype=np.float32)\n",
    "    qVec = np.zeros((batch_size, qmax), dtype=np.float32)        \n",
    "    for i in range(batch_size):\n",
    "        cVec[i, 0:ctx[i].shape[1]] = ctx[i]\n",
    "        qVec[i, 0:qn[i].shape[1]] = qn[i]\n",
    "    \n",
    "    return Variable(cVec), \\\n",
    "           Variable(qVec), \\\n",
    "           Variable(np.array(ans_start, dtype=np.int32)).reshape(-1,1), \\\n",
    "           Variable(np.array(ans_end, dtype=np.int32)).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "* RNN Tutorial: http://docs.chainer.org/en/stable/tutorial/recurrentnet.html\n",
    "* Training Tutorial: http://docs.chainer.org/en/stable/tutorial/train_loop.html\n",
    "* Attention: https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n",
    "* Pointer: http://fastml.com/introduction-to-pointer-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CoattentionEncoder(Chain):\n",
    "    def __init__(self, wordvec_size, h_size, use_gpu=False):\n",
    "        super(CoattentionEncoder, self).__init__()\n",
    "        \n",
    "        self.h_size = h_size\n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.ctxRU = L.LSTM(wordvec_size, h_size)\n",
    "\n",
    "            self.qnRU = L.LSTM(wordvec_size, h_size)\n",
    "            self.qnLinear = L.Linear(h_size, h_size)\n",
    "            \n",
    "            self.outFwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outBwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outLinear = L.Linear(2*h_size, h_size)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"CodynamicAttention uses GPU\", self.use_gpu\n",
    "                self.ctxRU.to_gpu()\n",
    "                self.qnRU.to_gpu()\n",
    "                self.qnLinear.to_gpu()\n",
    "                self.outFwd.to_gpu()\n",
    "                self.outBwd.to_gpu()\n",
    "                self.outLinear.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.ctxRU.reset_state()\n",
    "        self.qnRU.reset_state()\n",
    "        self.outFwd.reset_state()\n",
    "        self.outBwd.reset_state()\n",
    "        \n",
    "    def get_para_rep(self, para, ru):\n",
    "        P = []\n",
    "        for i in range(0, para.shape[1], self.wordvec_size):\n",
    "            word = para[:, i:i+self.wordvec_size]\n",
    "            if self.use_gpu: \n",
    "                word.to_gpu()\n",
    "            P.append(ru(word))\n",
    "        return F.transpose(F.dstack(P), (0, 1, 2))\n",
    "            \n",
    "    def __call__(self, ctx, qn):\n",
    "        # context representation\n",
    "        Ds = self.get_para_rep(ctx, self.ctxRU)\n",
    "        \n",
    "        #question representation\n",
    "        Qs = self.get_para_rep(qn, self.qnRU)\n",
    "        \n",
    "        out_ins = []\n",
    "        for i in range(Ds.shape[0]):\n",
    "            D = Ds[i]\n",
    "            Q = Qs[i]\n",
    "            \n",
    "            #attention\n",
    "            affinity = F.matmul(D.T, Q)\n",
    "            A_Q = F.softmax(affinity)\n",
    "            A_D = F.softmax(affinity.T)\n",
    "\n",
    "            C_Q = F.matmul(D, A_Q)\n",
    "            C_D = F.matmul(F.concat((Q, C_Q), axis=0), A_D)\n",
    "            \n",
    "            out_ins.append(F.concat((D, C_D), axis=0).T)\n",
    "        out_ins = F.transpose(F.dstack(out_ins), (0,2,1))\n",
    "\n",
    "        #output\n",
    "        h_fwd = []\n",
    "        for fout in out_ins:\n",
    "            h_fwd.append(self.outFwd(fout))\n",
    "        h_fwd = F.dstack(h_fwd)\n",
    "\n",
    "        h_bwd = []\n",
    "        for bout in out_ins[::-1]:\n",
    "            h_bwd.append(self.outBwd(bout))\n",
    "        h_bwd = F.dstack(h_bwd)\n",
    "        \n",
    "        u_in = F.transpose(F.concat((h_fwd, h_bwd)), (0,2,1))\n",
    "        U = self.outLinear(u_in.reshape(-1, 2*self.h_size))\n",
    "        return U.reshape(Ds.shape[0], -1, self.h_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 207, 35)\n"
     ]
    }
   ],
   "source": [
    "ctx, qn, ans_start, ans_end = get_batch(0, 5, train)\n",
    "\n",
    "encoder = CoattentionEncoder(WORD_VECTOR_SIZE, H_SIZE)\n",
    "\n",
    "U = encoder(ctx, qn)\n",
    "print U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Highway(Chain):\n",
    "    def __init__(self, h_size, use_gpu=False):\n",
    "        super(Highway, self).__init__()\n",
    "        \n",
    "        self.h_size = h_size\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.MLP = L.Linear(3*h_size, h_size, nobias=True)\n",
    "            self.M1 = L.Linear(2*h_size, h_size)\n",
    "            self.M2 = L.Linear(h_size, h_size)\n",
    "            self.M3 = L.Linear(2*h_size, 1)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"Highway uses GPU\", self.use_gpu\n",
    "                self.MLP.to_gpu()\n",
    "                self.M1.to_gpu()\n",
    "                self.M2.to_gpu()\n",
    "                self.M3.to_gpu()\n",
    "            \n",
    "    def __call__(self, U, h, us, ue):\n",
    "        if self.use_gpu:\n",
    "            U.to_gpu()\n",
    "            h.to_gpu()\n",
    "            us.to_gpu()\n",
    "            ue.to_gpu()\n",
    "        \n",
    "        r = F.tanh(self.MLP(F.hstack([h, us, ue])))\n",
    "        rs = []\n",
    "        for i in range(U.shape[0]):\n",
    "            rs.append(F.broadcast_to(r[i], U[i].shape))\n",
    "        r = F.transpose(F.dstack(rs), (2,0,1))\n",
    "        \n",
    "        m_in = F.concat((U, r), axis=2).reshape(-1, 2*self.h_size)\n",
    "        m1 = self.M1(m_in)\n",
    "        m2 = self.M2(m1)\n",
    "        m3 = self.M3(F.concat((m1,m2)))\n",
    "        \n",
    "        return m3.reshape(U.shape[0], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 207, 1)\n"
     ]
    }
   ],
   "source": [
    "highway = Highway(H_SIZE)\n",
    "\n",
    "h = Variable(np.zeros((5, H_SIZE), dtype=np.float32))\n",
    "us = U[:,0].reshape(5, -1)\n",
    "ue = U[:,-1].reshape(5, -1)\n",
    "\n",
    "alpha = highway(U, h, us, ue)\n",
    "print alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicPointingDecoder(Chain):\n",
    "    def __init__(self, h_size, use_gpu=False):\n",
    "        super(DynamicPointingDecoder, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.dec_state = L.LSTM(2*h_size, h_size)\n",
    "            self.HwayStart = Highway(h_size, use_gpu)\n",
    "            self.HwayEnd = Highway(h_size, use_gpu)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                print \"DynamicPointincDecoded uses GPU\", self.use_gpu\n",
    "                self.dec_state.to_gpu()\n",
    "                self.HwayStart.to_gpu()\n",
    "                self.HwayEnd.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.dec_state.reset_state()\n",
    "            \n",
    "    def __call__(self, U, us, ue):\n",
    "        if self.use_gpu:\n",
    "            U.to_gpu()\n",
    "            us.to_gpu()\n",
    "            ue.to_gpu()\n",
    "        \n",
    "        h = self.dec_state(F.concat((us,ue)))\n",
    "        alpha = self.HwayStart(U, h, us, ue)\n",
    "        s = F.argmax(alpha, axis=1).data.reshape(-1)\n",
    "        beta = self.HwayEnd(U, h, U[range(U.shape[0]), s], ue)\n",
    "        \n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 207, 1) [[202 152 168   0 201]]\n",
      "(5, 207, 1) [[ 1 79 65  0 54]]\n"
     ]
    }
   ],
   "source": [
    "decoder = DynamicPointingDecoder(H_SIZE)\n",
    "\n",
    "alpha, beta = decoder(U, us, ue)\n",
    "print alpha.shape, F.argmax(alpha, axis=1).data.reshape(1,-1)\n",
    "print beta.shape, F.argmax(beta, axis=1).data.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SquadNet(Chain):\n",
    "    def __init__(self, wordvec_size, h_size, use_gpu=False):\n",
    "        super(SquadNet, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.encoder = CoattentionEncoder(wordvec_size, h_size, use_gpu)\n",
    "            self.decoder = DynamicPointingDecoder(h_size, use_gpu)\n",
    "            \n",
    "            if use_gpu:\n",
    "                print \"SquadNet uses GPU\", self.use_gpu\n",
    "                self.encoder.to_gpu()\n",
    "                self.decoder.to_gpu()\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.encoder.reset_state()\n",
    "        self.decoder.reset_state()\n",
    "            \n",
    "    def __call__(self, ctx, qn): \n",
    "        U = self.encoder(ctx, qn)\n",
    "        \n",
    "        start = np.zeros(U.shape[0], 'i')\n",
    "        end = np.zeros(U.shape[0], 'i') - 1        \n",
    "        for i in range(3):            \n",
    "            us = U[range(U.shape[0]), start]\n",
    "            ue = U[range(U.shape[0]), end]\n",
    "            alpha, beta = self.decoder(U, us, ue)\n",
    "            \n",
    "            start = F.argmax(alpha, axis=1).data.reshape(-1)\n",
    "            end = F.argmax(beta, axis=1).data.reshape(-1)\n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 207, 1) [[205 109 182  29 206]]\n",
      "(5, 207, 1) [[ 78  78 155  45 108]]\n"
     ]
    }
   ],
   "source": [
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE)\n",
    "alpha, beta = model(ctx, qn)\n",
    "print alpha.shape, F.argmax(alpha, axis=1).data.reshape(1,-1)\n",
    "print beta.shape, F.argmax(beta, axis=1).data.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(alpha=1e-3)\n",
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE, USE_GPU)\n",
    "if USE_GPU:\n",
    "    model.to_gpu()\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, opt, epoch_start, epoch_end, batch_size, print_interval):\n",
    "    for epoch in range(epoch_start, epoch_end):\n",
    "        print \"Epoch\", epoch + 1, \"/\", epoch_end\n",
    "        startTime = time.time()\n",
    "        epochScore = 0\n",
    "\n",
    "        opt.new_epoch()\n",
    "        \n",
    "        interval_loss = 0\n",
    "        interval_start = time.time()\n",
    "        for i in range(0, len(train), batch_size):\n",
    "            try:\n",
    "                ctx, qn, ans_start, ans_end = get_batch(i, batch_size, train)\n",
    "                if USE_GPU:\n",
    "                    ans_start.to_gpu()\n",
    "                    ans_end.to_gpu()\n",
    "\n",
    "                model.reset_state()\n",
    "                pred_start, pred_end = model(ctx, qn)\n",
    "\n",
    "                loss_start = F.softmax_cross_entropy(pred_start, ans_start)\n",
    "                loss_end = F.softmax_cross_entropy(pred_end, ans_end)\n",
    "                loss = loss_start + loss_end\n",
    "\n",
    "                interval_loss += loss.data\n",
    "                if i % print_interval == 0:\n",
    "                    print i, \"/\", len(train), \":\", \\\n",
    "                          interval_loss, \\\n",
    "                          \"(\" + str(time.time() - interval_start) + \"s)\"\n",
    "                    interval_loss = 0\n",
    "                    interval_start = time.time()\n",
    "                \n",
    "                s = F.argmax(pred_start, axis=1).data\n",
    "                e = F.argmax(pred_end, axis=1).data\n",
    "                for j in range(s.shape[0]):\n",
    "                    if s[j] == ans_start.data[j] and e[j] == ans_end.data[j]:\n",
    "                        epochScore += 1\n",
    "\n",
    "                model.cleargrads()\n",
    "                loss.backward()\n",
    "\n",
    "                opt.update()\n",
    "            except IndexError as e:\n",
    "                print \"Error on train index \" + str(i) + \":\", e\n",
    "        \n",
    "        valLoss = 0\n",
    "        valScore = 0\n",
    "        for i in range(0, len(val), batch_size):\n",
    "            try:\n",
    "                ctx, qn, ans_start, ans_end = get_batch(i, batch_size, val)\n",
    "                if USE_GPU:\n",
    "                    ans_start.to_gpu()\n",
    "                    ans_end.to_gpu()\n",
    "\n",
    "                model.reset_state()\n",
    "                pred_start, pred_end = model(ctx, qn)\n",
    "\n",
    "                loss_start = F.softmax_cross_entropy(pred_start, ans_start)\n",
    "                loss_end = F.softmax_cross_entropy(pred_end, ans_end)\n",
    "                valLoss += loss_start + loss_end\n",
    "                \n",
    "                s = F.argmax(pred_start, axis=1).data\n",
    "                e = F.argmax(pred_end, axis=1).data\n",
    "                for j in range(s.shape[0]):\n",
    "                    if s[j] == ans_start.data[j] and e[j] == ans_end.data[j]:\n",
    "                        valScore += 1\n",
    "            except IndexError as e:\n",
    "                print \"Error on val index \" + str(i) + \":\", e\n",
    "        \n",
    "        epochAcc = float(epochScore) / len(train)\n",
    "        valAcc = float(valScore) / len(val)\n",
    "        \n",
    "        serializers.save_npz('gpu-epoch' + str(epoch+1) + '.model', model)\n",
    "        print \"Epoch completed in\", time.time() - startTime, \"seconds\"\n",
    "        print \"Train Acc:\", epochAcc, \"Val Acc:\", valAcc, \"Val Loss:\", valLoss      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\n",
      "0 / 200 : 9.1655960083 (1.29957699776s)\n",
      "100 / 200 : 18.3419818878 (11.838588953s)\n",
      "Epoch completed in 34.8875339031 seconds\n",
      "Train Acc: 0.02 Val Acc: 0.01 Val Loss: variable(9.223145484924316)\n",
      "Epoch 2 / 1\n",
      "0 / 200 : 9.01286506653 (1.32164788246s)\n",
      "100 / 200 : 17.8375654221 (11.5460650921s)\n",
      "Epoch completed in 33.1986849308 seconds\n",
      "Train Acc: 0.01 Val Acc: 0.01 Val Loss: variable(9.216375350952148)\n"
     ]
    }
   ],
   "source": [
    "train_model(model, opt, 0, 1, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serializers.save_npz('one_epoch.model', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_batch(i, batch_size, data):\n",
    "    j = min(i + batch_size, len(data))\n",
    "    \n",
    "    ctx = []\n",
    "    qn = []\n",
    "    ids = []\n",
    "    ctxStrs = []\n",
    "    \n",
    "    cmax = 0\n",
    "    qmax = 0\n",
    "    for k in range(i, j):\n",
    "        c, q, s, e = data[k]\n",
    "        ctx.append(c)\n",
    "        qn.append(q)\n",
    "        ids.append(s)\n",
    "        ctxStrs.append(e)\n",
    "        \n",
    "        cmax = max(cmax, c.shape[1])\n",
    "        qmax = max(cmax, q.shape[1])\n",
    "\n",
    "    cVec = np.zeros((batch_size, cmax), dtype=np.float32)\n",
    "    qVec = np.zeros((batch_size, qmax), dtype=np.float32)        \n",
    "    for i in range(batch_size):\n",
    "        cVec[i, 0:ctx[i].shape[1]] = ctx[i]\n",
    "        qVec[i, 0:qn[i].shape[1]] = qn[i]\n",
    "    \n",
    "    return Variable(cVec), \\\n",
    "           Variable(qVec), \\\n",
    "           ids, \\\n",
    "           ctxStrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serializers.load_npz('one_epoch.model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200 (3.02272391319s)\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 100\n",
    "test_print_interval = 1000\n",
    "\n",
    "f = open('pred.csv', 'wb')\n",
    "out = csv.writer(f)\n",
    "out.writerow([\"Id\", \"Answer\"])\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "for i in range(0, len(test), test_batch_size):\n",
    "    ctx, qn, qnId, ctxStr = get_test_batch(i, test_batch_size, test)\n",
    "    model.reset_state()\n",
    "    start, end = model(ctx, qn)\n",
    "\n",
    "    for j in range(len(qnId)):\n",
    "        contextTokens = word_tokenize(ctxStr[j])\n",
    "\n",
    "        s = F.argmax(start[j]).data\n",
    "        e = F.argmax(end[j]).data\n",
    "        \n",
    "        s = min(s, len(contextTokens)-1)\n",
    "        e = max(e, s)\n",
    "        e = min(e, len(contextTokens)-1)        \n",
    "        \n",
    "        ans = \"\"\n",
    "        for k in range(s, e + 1):\n",
    "            ans += contextTokens[k] + \" \"\n",
    "        \n",
    "        out.writerow([qnId[j], normalize_answer(ans).encode('utf-8')])\n",
    "    \n",
    "    if i % test_print_interval == 0:\n",
    "        print i, \"/\", len(test), \"(\" + str(time.time() - startTime) + \"s)\"\n",
    "        startTime = time.time()\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
