{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv, json, string, re, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from chainer import Chain, Variable, Parameter\n",
    "from chainer import iterators, optimizers\n",
    "import chainer.initializers as I\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "WORD_VECTOR_SIZE = 50\n",
    "H_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = {}\n",
    "f = open('glove/glove.6B.' + str(WORD_VECTOR_SIZE) + 'd.txt', 'rb')\n",
    "reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "for row in reader:\n",
    "    key = row[0]\n",
    "    vector = map(float, row[1:])\n",
    "    glove[key] = np.array(vector, dtype=np.float32).reshape(1,-1)\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    textVec = []\n",
    "    for tok in tokens:\n",
    "        textVec.append(glove.get(tok, np.zeros((1,WORD_VECTOR_SIZE), dtype=np.float32)))\n",
    "    return np.vstack(textVec)\n",
    "\n",
    "def answerpos(context, answer, answer_start):\n",
    "    start = len(word_tokenize(context[:answer_start]))\n",
    "    ans_len = len(word_tokenize(answer))\n",
    "    \n",
    "    return start, start + ans_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61379\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for jsonRow in json.loads(open('dataset/train.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']: \n",
    "        ctxLen = len(paragraph['context'])\n",
    "        ctxVec = text2vec(paragraph['context'])\n",
    "        \n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qnVec = text2vec(qnaJson['question'].lower())\n",
    "            \n",
    "            ansStart, ansEnd = answerpos(paragraph['context'], \n",
    "                                           qnaJson['answer']['text'], \n",
    "                                           qnaJson['answer']['answer_start'])\n",
    "            \n",
    "            train.append((ctxVec, qnVec, ansStart, ansEnd))\n",
    "print len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7984\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for jsonRow in json.loads(open('dataset/test.json', 'rb').read()):\n",
    "    for paragraph in jsonRow['paragraphs']: \n",
    "        data = {}\n",
    "        data['context'] = paragraph['context']\n",
    "        data['contextVec'] = text2vec(paragraph['context'])\n",
    "        data['qna'] = []\n",
    "        for qnaJson in paragraph['qas']:\n",
    "            qna = {}\n",
    "            qna['id'] = qnaJson['id']\n",
    "            qna['question'] = qnaJson['question']\n",
    "            qna['questionVec'] = text2vec(qnaJson['question'].lower())\n",
    "            data['qna'].append(qna)\n",
    "        test.append(data)\n",
    "\n",
    "print len(test)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "* RNN Tutorial: http://docs.chainer.org/en/stable/tutorial/recurrentnet.html\n",
    "* Training Tutorial: http://docs.chainer.org/en/stable/tutorial/train_loop.html\n",
    "* Attention: https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n",
    "* Pointer: http://fastml.com/introduction-to-pointer-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CoattentionEncoder(Chain):\n",
    "    def __init__(self, wordvec_size, h_size):\n",
    "        super(CoattentionEncoder, self).__init__()\n",
    "        \n",
    "        self.h_size = h_size\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.ctxRU = L.LSTM(wordvec_size, h_size)\n",
    "\n",
    "            self.qnRU = L.LSTM(wordvec_size, h_size)\n",
    "            self.qnLinear = L.Linear(h_size, h_size)\n",
    "            \n",
    "            self.outFwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outBwd = L.LSTM(3*h_size, h_size)\n",
    "            self.outLinear = L.Linear(2*h_size, h_size)\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.ctxRU.reset_state()\n",
    "        self.qnRU.reset_state()\n",
    "        self.outFwd.reset_state()\n",
    "        self.outBwd.reset_state()\n",
    "            \n",
    "    def __call__(self, context, question):\n",
    "        # context representation\n",
    "        D = self.ctxRU(context).T\n",
    "        \n",
    "        #question representation\n",
    "        Q = self.qnRU(question).T\n",
    "        \n",
    "        #attention\n",
    "        affinity = F.matmul(D.T, Q)\n",
    "        A_Q = F.softmax(affinity)\n",
    "        A_D = F.softmax(affinity.T)\n",
    "        \n",
    "        C_Q = F.matmul(D, A_Q)\n",
    "        C_D = F.matmul(F.concat((Q, C_Q), axis=0), A_D)\n",
    "        \n",
    "        #output\n",
    "        out_in = F.concat((D, C_D), axis=0).T        \n",
    "        h_fwd = self.outFwd(out_in)\n",
    "        h_bwd = self.outBwd(out_in[::-1])\n",
    "    \n",
    "        U = self.outLinear(F.concat((h_fwd, h_bwd)))\n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 200)\n",
      "variable([[-0.01496673 -0.01889673 -0.00773166 ..., -0.03226301 -0.01829917\n",
      "            0.03382658]\n",
      "          [-0.01490557 -0.0423511  -0.01553484 ..., -0.03195731 -0.01306379\n",
      "            0.01433437]\n",
      "          [-0.00751606 -0.02998942 -0.00652336 ..., -0.02165518 -0.03221701\n",
      "            0.02800097]\n",
      "          ..., \n",
      "          [-0.02748947 -0.01415092  0.00182293 ..., -0.01300309 -0.02445512\n",
      "            0.02127449]\n",
      "          [-0.0307134  -0.0468216  -0.00205125 ...,  0.01821324 -0.01728526\n",
      "            0.02641234]\n",
      "          [-0.02064704 -0.0097578   0.00653811 ...,  0.00406266  0.00351155\n",
      "            0.01039272]])\n"
     ]
    }
   ],
   "source": [
    "ctx = train[0][0]\n",
    "qn = train[0][1]\n",
    "encoder = CoattentionEncoder(WORD_VECTOR_SIZE, H_SIZE)\n",
    "\n",
    "U = encoder(ctx, qn)\n",
    "print U.shape\n",
    "print U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Highway(Chain):\n",
    "    def __init__(self, h_size):\n",
    "        super(Highway, self).__init__()\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.MLP = L.Linear(3*h_size, h_size, nobias=True)\n",
    "            self.M1 = L.Linear(2*h_size, h_size)\n",
    "            self.M2 = L.Linear(h_size, h_size)\n",
    "            self.M3 = L.Linear(2*h_size, 1)\n",
    "            \n",
    "    def __call__(self, U, h, us, ue):\n",
    "        r = F.tanh(self.MLP(F.hstack([h, us, ue])))\n",
    "        m1 = self.M1(F.concat((U, F.broadcast_to(r, U.shape))))\n",
    "        m2 = self.M2(m1)\n",
    "        m3 = self.M3(F.concat((m1,m2)))\n",
    "        \n",
    "        return m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 1)\n"
     ]
    }
   ],
   "source": [
    "highway = Highway(H_SIZE)\n",
    "\n",
    "h = np.zeros((1,H_SIZE), dtype=np.float32)\n",
    "us = U[0].reshape(1,-1)\n",
    "ue = U[-1].reshape(1,-1)\n",
    "\n",
    "alpha = highway(U, h, us, ue)\n",
    "print alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicPointingDecoder(Chain):\n",
    "    def __init__(self, h_size):\n",
    "        super(DynamicPointingDecoder, self).__init__()\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.dec_state = L.LSTM(2*h_size, h_size)\n",
    "            self.HwayStart = Highway(h_size)\n",
    "            self.HwayEnd = Highway(h_size)\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.dec_state.reset_state()\n",
    "            \n",
    "    def __call__(self, U, us, ue):\n",
    "        h = self.dec_state(F.concat((us,ue)))\n",
    "        alpha = highway(U, h, us, ue)\n",
    "        s = F.argmax(alpha).data\n",
    "        beta = highway(U, h, U[s].reshape(1,-1), ue)\n",
    "        \n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 1) variable(68)\n",
      "(125, 1) variable(68)\n"
     ]
    }
   ],
   "source": [
    "decoder = DynamicPointingDecoder(H_SIZE)\n",
    "\n",
    "us = U[0].reshape(1,-1)\n",
    "ue = U[-1].reshape(1,-1)\n",
    "alpha, beta = decoder(U, us, ue)\n",
    "print alpha.shape, F.argmax(alpha)\n",
    "print beta.shape, F.argmax(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SquadNet(Chain):\n",
    "    def __init__(self, wordvec_size, h_size):\n",
    "        super(SquadNet, self).__init__()\n",
    "                \n",
    "        with self.init_scope():\n",
    "            self.encoder = CoattentionEncoder(wordvec_size, h_size)\n",
    "            self.decoder = DynamicPointingDecoder(h_size)\n",
    "            \n",
    "    def reset_state(self):\n",
    "        self.encoder.reset_state()\n",
    "        self.decoder.reset_state()\n",
    "            \n",
    "    def __call__(self, ctx, qn):\n",
    "        U = self.encoder(ctx, qn)\n",
    "        \n",
    "        start = 0\n",
    "        end = -1\n",
    "        for i in range(3):            \n",
    "            us = U[start].reshape(1, -1)\n",
    "            ue = U[end].reshape(1, -1)\n",
    "            alpha, beta = self.decoder(U, us, ue)\n",
    "            \n",
    "            start = F.argmax(alpha).data\n",
    "            end = F.argmax(beta).data\n",
    "        return alpha.reshape(1, -1), beta.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 1)\n",
      "(125, 1)\n"
     ]
    }
   ],
   "source": [
    "ctx = train[0][0]\n",
    "qn = train[0][1]\n",
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE)\n",
    "alpha. beta = model(ctx, qn)\n",
    "print alpha.shape\n",
    "print beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = optimizers.Adam()\n",
    "model = SquadNet(WORD_VECTOR_SIZE, H_SIZE)\n",
    "\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2\n",
      "0 / 61379 : 1.79932117462 (0.0678100585938s)\n",
      "1000 / 61379 : 6862.71748328 (128.100099087s)\n",
      "2000 / 61379 : 7859.78378582 (133.302760839s)\n",
      "3000 / 61379 : 7555.04627228 (138.290160179s)\n",
      "4000 / 61379 : 7009.47757578 (135.738970995s)\n",
      "5000 / 61379 : 7217.49965477 (136.134599924s)\n",
      "6000 / 61379 : 7363.99676085 (134.371236086s)\n",
      "7000 / 61379 : 7102.01323843 (132.691280842s)\n",
      "8000 / 61379 : 8033.76777077 (145.369439125s)\n",
      "9000 / 61379 : 7496.97219038 (136.565308094s)\n",
      "10000 / 61379 : 7653.9298625 (140.999716997s)\n",
      "11000 / 61379 : 7049.08306122 (129.703157902s)\n",
      "12000 / 61379 : 8271.8430419 (149.124598026s)\n",
      "13000 / 61379 : 7726.87153149 (126.37846303s)\n",
      "14000 / 61379 : 8001.32679319 (142.517619133s)\n",
      "15000 / 61379 : 7800.00272036 (140.180574894s)\n",
      "16000 / 61379 : 7467.89545536 (134.119723082s)\n",
      "17000 / 61379 : 7262.11009264 (142.394720078s)\n",
      "18000 / 61379 : 7366.69743681 (144.496595144s)\n",
      "19000 / 61379 : 7503.78264952 (135.446516991s)\n",
      "20000 / 61379 : 7247.93611956 (139.518861055s)\n",
      "21000 / 61379 : 7149.74093533 (128.038532972s)\n",
      "22000 / 61379 : 7681.51847744 (143.18385005s)\n",
      "23000 / 61379 : 7204.50403214 (141.127509832s)\n",
      "24000 / 61379 : 7461.00825214 (144.540717125s)\n",
      "25000 / 61379 : 7957.99427986 (140.169983149s)\n",
      "26000 / 61379 : 7653.92665005 (135.109466076s)\n",
      "27000 / 61379 : 7579.22167492 (155.035727024s)\n",
      "28000 / 61379 : 7096.74591255 (131.20620203s)\n",
      "29000 / 61379 : 7367.07558966 (136.141510963s)\n",
      "30000 / 61379 : 7611.65060043 (137.557641029s)\n",
      "31000 / 61379 : 7649.11207581 (140.09291482s)\n",
      "32000 / 61379 : 7383.31805897 (145.722221851s)\n",
      "33000 / 61379 : 6582.54854918 (121.086607933s)\n",
      "34000 / 61379 : 7349.73099899 (133.999162912s)\n",
      "35000 / 61379 : 7571.87543821 (170.821167946s)\n",
      "36000 / 61379 : 7933.91399622 (129.489377022s)\n",
      "37000 / 61379 : 7463.67699385 (140.838702202s)\n",
      "38000 / 61379 : 7797.92691326 (139.93471694s)\n",
      "39000 / 61379 : 7329.97745609 (127.163851023s)\n",
      "40000 / 61379 : 7423.0644598 (143.906516075s)\n",
      "41000 / 61379 : 6524.23997974 (127.101097822s)\n",
      "42000 / 61379 : 7282.00149918 (141.625986099s)\n",
      "43000 / 61379 : 7304.1882925 (137.767323017s)\n",
      "44000 / 61379 : 7444.41973686 (130.603332996s)\n",
      "45000 / 61379 : 7386.83827591 (138.744167089s)\n",
      "46000 / 61379 : 7578.23117924 (132.87124896s)\n",
      "47000 / 61379 : 7404.00450373 (124.000823975s)\n",
      "48000 / 61379 : 7491.26022959 (134.769087791s)\n",
      "49000 / 61379 : 7081.83950996 (115.900625944s)\n",
      "50000 / 61379 : 6763.15419912 (112.660655022s)\n",
      "51000 / 61379 : 7061.16422844 (124.605101824s)\n",
      "52000 / 61379 : 7458.65489864 (136.720088959s)\n",
      "Error on index 52553: index 102 is out of bounds for axis 0 with size 102\n",
      "53000 / 61379 : 7371.86135244 (141.308801889s)\n",
      "54000 / 61379 : 6430.57760715 (129.318976879s)\n",
      "55000 / 61379 : 7316.8926096 (142.179727077s)\n",
      "56000 / 61379 : 7284.49628448 (149.027155161s)\n",
      "57000 / 61379 : 7392.9968133 (141.650574207s)\n",
      "58000 / 61379 : 7108.12847042 (126.920892954s)\n",
      "59000 / 61379 : 6623.85097122 (128.268854856s)\n",
      "60000 / 61379 : 7295.38622379 (128.452393055s)\n",
      "61000 / 61379 : 7423.43694019 (144.286417961s)\n",
      "Epoch completed in 8368.58051515 seconds\n",
      "Epoch 2 / 2\n",
      "0 / 61379 : 1.68734169006 (0.0433878898621s)\n",
      "1000 / 61379 : 6802.85232925 (126.298296928s)\n",
      "2000 / 61379 : 7808.65605736 (132.504247189s)\n",
      "3000 / 61379 : 7473.82368898 (137.340217113s)\n",
      "4000 / 61379 : 6931.79460716 (135.275123835s)\n",
      "5000 / 61379 : 7157.961411 (134.922403097s)\n",
      "6000 / 61379 : 7243.87854338 (134.905622005s)\n",
      "7000 / 61379 : 7044.3430624 (132.798180103s)\n",
      "8000 / 61379 : 7952.37261868 (145.44336915s)\n",
      "9000 / 61379 : 7435.95141268 (136.467041969s)\n",
      "10000 / 61379 : 7620.07644701 (141.248562098s)\n",
      "11000 / 61379 : 6961.41942406 (129.685863972s)\n",
      "12000 / 61379 : 8229.14483547 (149.127939939s)\n",
      "13000 / 61379 : 7667.80740261 (125.789026976s)\n",
      "14000 / 61379 : 7927.72624922 (142.112639189s)\n",
      "15000 / 61379 : 7714.56003642 (139.456958055s)\n",
      "16000 / 61379 : 7404.24005759 (134.332645893s)\n",
      "17000 / 61379 : 7217.00346994 (142.147573948s)\n",
      "18000 / 61379 : 7310.73822069 (144.538245916s)\n",
      "19000 / 61379 : 7438.05552244 (135.338917017s)\n",
      "20000 / 61379 : 7199.14200163 (139.635792017s)\n",
      "21000 / 61379 : 7152.98870659 (128.167732954s)\n",
      "22000 / 61379 : 7564.51576233 (143.638284922s)\n",
      "23000 / 61379 : 7151.84934807 (141.070447922s)\n",
      "24000 / 61379 : 7444.48622799 (144.689246893s)\n",
      "25000 / 61379 : 7901.78564358 (140.332452059s)\n",
      "26000 / 61379 : 7610.47532654 (135.153637171s)\n",
      "27000 / 61379 : 7524.07486057 (155.363303185s)\n",
      "28000 / 61379 : 7021.30919838 (131.206221819s)\n",
      "29000 / 61379 : 7311.79125786 (136.391027927s)\n",
      "30000 / 61379 : 7528.26351833 (137.489253998s)\n",
      "31000 / 61379 : 7580.47634792 (140.143995047s)\n",
      "32000 / 61379 : 7303.49138165 (146.772532225s)\n",
      "33000 / 61379 : 6521.55327606 (121.694248915s)\n",
      "34000 / 61379 : 7331.90129852 (134.474324942s)\n",
      "35000 / 61379 : 7535.59704304 (178.19986105s)\n",
      "36000 / 61379 : 7904.74445438 (130.729562044s)\n",
      "37000 / 61379 : 7403.62680626 (142.829598904s)\n",
      "38000 / 61379 : 7756.08017159 (141.332161903s)\n",
      "39000 / 61379 : 7287.53194809 (128.588187218s)\n",
      "40000 / 61379 : 7359.65223408 (143.883567095s)\n",
      "41000 / 61379 : 6453.43054676 (126.840357065s)\n",
      "42000 / 61379 : 7195.31537056 (141.93945694s)\n",
      "43000 / 61379 : 7256.66007233 (138.084997892s)\n",
      "44000 / 61379 : 7387.2051363 (130.769685984s)\n",
      "45000 / 61379 : 7362.98578262 (139.0336411s)\n",
      "46000 / 61379 : 7508.24614239 (133.69265008s)\n",
      "47000 / 61379 : 7317.62745476 (131.792913198s)\n",
      "48000 / 61379 : 7443.02801323 (135.957688808s)\n",
      "49000 / 61379 : 7003.251194 (116.100805998s)\n",
      "50000 / 61379 : 6657.23833084 (113.01721096s)\n",
      "51000 / 61379 : 7031.00878811 (124.944682837s)\n",
      "52000 / 61379 : 7413.6381321 (140.046154976s)\n",
      "Error on index 52553: index 102 is out of bounds for axis 0 with size 102\n",
      "53000 / 61379 : 7343.29487419 (141.44066s)\n",
      "54000 / 61379 : 6393.26625347 (130.089080095s)\n",
      "55000 / 61379 : 7245.55927372 (142.854635s)\n",
      "56000 / 61379 : 7241.63983631 (149.819687843s)\n",
      "57000 / 61379 : 7322.91051197 (140.735622883s)\n",
      "58000 / 61379 : 7067.79939556 (128.009860039s)\n",
      "59000 / 61379 : 6541.65506172 (152.487700939s)\n",
      "60000 / 61379 : 7234.15455532 (143.560550928s)\n",
      "61000 / 61379 : 7383.07078648 (165.458615065s)\n",
      "Epoch completed in 8464.12861991 seconds\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 2\n",
    "PRINT_INTERVAL = 1000\n",
    "\n",
    "len_train = len(train)\n",
    "for epoch in range(N_EPOCH):\n",
    "    print \"Epoch\", epoch + 1, \"/\", N_EPOCH\n",
    "    startTime = time.time()\n",
    "    \n",
    "    interval_loss = 0\n",
    "    interval_start = time.time()\n",
    "    for i in range(len_train):\n",
    "        try:\n",
    "            ctx, qn, ans_start, ans_end = train[i]\n",
    "\n",
    "            model.reset_state()\n",
    "            pred_start, pred_end = model(ctx, qn)\n",
    "\n",
    "            loss_start = F.softmax_cross_entropy(pred_start, np.array([ans_start], dtype=np.int32))\n",
    "            loss_end = F.softmax_cross_entropy(pred_end, np.array([ans_end], dtype=np.int32))\n",
    "            loss = loss_start + loss_end\n",
    "            \n",
    "            interval_loss += loss.data\n",
    "            if i % PRINT_INTERVAL == 0:\n",
    "                print i, \"/\", len_train, \":\", interval_loss, \"(\" + str(time.time() - interval_start) + \"s)\"\n",
    "                interval_loss = 0\n",
    "                interval_start = time.time()\n",
    "\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            opt.update()\n",
    "        except IndexError as e:\n",
    "            print \"Error on index \" + str(i) + \":\", e\n",
    "    print \"Epoch completed in\", time.time() - startTime, \"seconds\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('pred.csv', 'wb')\n",
    "out = csv.writer(f)\n",
    "out.writerow([\"Id\", \"Answer\"])\n",
    "\n",
    "for t in test:\n",
    "    ctx = word_tokenize(t['context'])\n",
    "    ctxVec = t['contextVec']\n",
    "    \n",
    "    for qna in t['qna']:\n",
    "        id = qna['id']\n",
    "        qn = qna['question']\n",
    "        qnVec = qna['questionVec']\n",
    "        \n",
    "        model.reset_state()\n",
    "        start, end = model(ctxVec, qnVec)\n",
    "        start = F.argmax(start).data\n",
    "        end = F.argmax(end).data\n",
    "        \n",
    "        ans = \"\"\n",
    "        for i in range(start, end + 1):\n",
    "            ans += ctx[i] + \" \"\n",
    "        \n",
    "        out.writerow([id, normalize_answer(ans).encode('utf-8')])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
